{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ECU/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "loading configuration file generation_config.json from cache at aitextgen/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aitextgen import aitextgen\n",
    "\n",
    "ai = aitextgen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are many reasons why the United Nations Development Programme (UNDP) has been unable to achieve the necessary level of development assistance to countries in need of assistance.\n",
      "\n",
      "The UNDP is the government-controlled body responsible for promoting better sustainable development. It has the responsibility to make sure that the needs of developing countries are met through their participation in the UN Development Programme. Since 2007, about 25 per cent of its funding has been allocated to developing countries.\n",
      "\n",
      "UNDP is a major donor to various states around the world. It is responsible for providing services to over 3,000 countries and territories. It also provides technical assistance to aid organisations such as UNICEF, UNICEF's International Development Programme and UNICEF's International Development and Development Programme.\n",
      "\n",
      "The UNDP is a member of the United Nations Development Programme (UNDP). It is responsible for its own development assistance programmes. It is also a member of the World Economic Forum (WEMF) and participates in the UN Economic Action Plan, which is being developed by the UN.\n",
      "\n",
      "UNDP is the world's largest private investor in the world's largest private and public sector companies. It has invested more than $8 billion in private equity and private equity investments since 1996.\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mUnicorns are amazing\u001b[0m.\n",
      "Re' the bit's could be dead,\n",
      "Alset inftersomes.\n",
      "See and I can be speed, all the pride.\n",
      "\n",
      "Clown:\n",
      "Rel forth\n",
      "==========\n",
      "\u001b[1mUnicorns are amazing\u001b[0m curem'd,\n",
      "The chard seen back'd in treason.\n",
      "\n",
      "SLYORK:\n",
      "TRANIO:\n",
      "Well, that must be your own age\n",
      "Madam'd intomen\n",
      "==========\n",
      "\u001b[1mUnicorns are amazing\u001b[0m\n",
      "Whose star's expatch.\n",
      "\n",
      "MENENIUS:\n",
      "Farm, no more.\n",
      "\n",
      "BRUTUS:\n",
      "I have heardonly:\n",
      "To-night;\n",
      "I see the prom the stars\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt = \"Unicorns are amazing\", n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWhat is sentience?\u001b[0m\n",
      "\n",
      "The first thing you need to know is that there are two kinds of signals that go on within your brain. The first is how long you delay in getting the information, which is called the delayed information. The second is how long you delay in getting the information (what you are saying).\n",
      "\n",
      "The delayed information is information that you are processing in a particular way at a specific time or time. These signals are called delayed information.\n",
      "\n",
      "When you are waiting for the information to be processed, you are sending signals to your neurons, as they respond to information you are sending to them.\n",
      "\n",
      "This means that your brain is sending signals that it knows you are processing, and if you wait too long, your brain is shutting down.\n",
      "\n",
      "That is why people with autism have this kind of delayed information.\n",
      "\n",
      "And that is why they have this kind of delayed information, which has to do with the fact that your brain is shutting down at a certain time.\n",
      "\n",
      "You will also notice that you have more delayed information than you think.\n",
      "\n",
      "When people with autism have this kind of delayed information, their brains are shut down.\n",
      "\n",
      "But when you wait too long, your brain can't keep up with what you\n",
      "==========\n",
      "\u001b[1mWhat is sentience?\u001b[0m\n",
      "\n",
      "In addition to knowing what kinds of things have been sent, we also know how to respond to those things. So when you're looking at the things that have been sent, you can be more specific and say, \"What's that thing that you saw?\"\n",
      "\n",
      "It's possible to be more specific and say, \"What are you seeing?\"\n",
      "\n",
      "That's a very important question.\n",
      "\n",
      "That's what I'm trying to do, and that's the thing that I'm trying to do for this book.\n",
      "\n",
      "So the question, you asked, is it possible to be more specific and say that this is like something that you see?\n",
      "\n",
      "I'll tell you this, you can't be more specific and say something like that. If you don't have the information, then you don't have the ability to respond to it.\n",
      "\n",
      "That's my big question, is it possible to be more specific and say that this is something that you see?\n",
      "\n",
      "It's not possible to be more specific and say that this is something that you see.\n",
      "\n",
      "It's not possible to be more specific and say that this is something that you see.\n",
      "\n",
      "So it doesn't make sense for you to say\n",
      "==========\n",
      "\u001b[1mWhat is sentience?\u001b[0m\n",
      "\n",
      "When you are in the middle of a struggle, you can receive information or other emotions. This is called the \"chosen\" emotion. If you are in a fight, you will receive information. In your next fight, you may receive information as well. In any fight, you have to fight to get information.\n",
      "\n",
      "If you are in the middle of a fight, you will receive information. This is called \"chosen\" emotion. If you are in a fight, you will receive information. In your next fight, you may receive information as well. In any fight, you have to fight to get information. The ability to receive information from others increases because information is available to you in a certain way. For example, you could receive information from a friend who is interested in you.\n",
      "\n",
      "You can receive information from others increases because information is available to you in a certain way. For example, you could receive information from a friend who is interested in you. The ability to receive information from someone else increases because information is available to you in a certain way. For example, you could receive information from an unknown person.\n",
      "\n",
      "You can receive information from someone else increases because information is available to you in a certain way\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt = \"What is sentience?\", n = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAre you an AI?\u001b[0m I hope you'll help us solve this problem.\n",
      "\n",
      "I'm a programmer. I've been writing software for a long time. I think that, for me, it has been a little weird to work at a company that doesn't have a lot of people who are programmers. It's a really weird company. It was interesting to hear from them about these startups, and I thought it was important to learn about this company, too. I tried to do that. I tried to be really good at it, and I realized that I didn't have to be a programmer. I was just looking for something that would help me.\n",
      "\n",
      "I wanted a company that could really help me. And that's what I wanted: an AI company.\n",
      "\n",
      "I was really interested in a company that could help me. I didn't know much about the technology, and I had never heard about any of the startups that I was involved in. But I was really interested in being a part of the company, and that's what I got involved in.\n",
      "\n",
      "We started working on the last project, and there were some questions about how we could get people interested in the tech. It's not going to happen anytime soon, and we want to\n",
      "==========\n",
      "\u001b[1mAre you an AI?\u001b[0m\n",
      "\n",
      "We're always looking for new ways to help you and our community. If you're interested in joining our team, please email [email protected] and you can also find out about other AI companies in the community.\n",
      "\n",
      "You can also join our community on twitter where you can see the community's progress.\n",
      "\n",
      "We look forward to hearing from you!\n",
      "\n",
      "–\n",
      "\n",
      "The Team\n",
      "\n",
      "Ryan\n",
      "\n",
      "Founder, CEO & Co-Founder, and Co-founder & CEO of the Foundation\n",
      "\n",
      "Ryan is a Certified Industrial Designer who has spent the last 24 years working as a Industrial Design Consultant (IDC). He has a Masters Degree in Industrial Design from the University of Texas at Austin and completed his degree in Industrial Design from the University of Texas at Austin. Ryan holds a Master of Science in Industrial Design from the University of Texas at Austin and also a Master of Science in Industrial Design from the University of Texas at Austin. Ryan also worked as a Senior Architect for the company's original mobile app, Mobile Design Studio.\n",
      "\n",
      "He is the co-founder of the Foundation's AI Lab and is the co-founder and CEO of the Foundation's IoT Laboratory.\n",
      "\n",
      "Follow us on Twitter for the latest\n",
      "==========\n",
      "\u001b[1mAre you an AI?\u001b[0m Are you an AI?\n",
      "\n",
      "I'm a computer scientist who has been doing research into the dynamics of the computer's behavior since early 1987. I've been working on the project for almost two years now and am just getting started. I'm not an engineer, I'm just an experienced computer scientist who is working with a team of researchers who are working on the project.\n",
      "\n",
      "You have a research career, what do you do?\n",
      "\n",
      "I'm a computer scientist at the University of California, Berkeley. I've been working on a project to test a new type of computerized neural network. I'm here to work on that project.\n",
      "\n",
      "You're one of the first researchers to use HCI as a model for artificial intelligence. What is the challenge of designing a data set that can handle data that is so large?\n",
      "\n",
      "That's something that I love to do. It's not just the size of the data set, but the size of the data set as a whole. I'm the best researcher on it.\n",
      "\n",
      "What makes you think you know the right way to use RNNs?\n",
      "\n",
      "I'm interested in the relationship between the language, the data, and the data. I'm interested in how you\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt = \"Are you an AI?\", n = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokenizer('input.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [04:50<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 87927.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = GPT2ConfigCPU()\n",
    "\n",
    "ai = aitextgen(tokenizer_file='aitextgen.tokenizer.json', config=config)\n",
    "\n",
    "data = TokenDataset('input.txt', tokenizer_file='aitextgen.tokenizer.json', block_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ECU/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:466: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ECU/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loss: 3.480 — Avg: 3.497: 100%|██████████| 5000/5000 [03:17<00:00, 25.28it/s]\n",
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m                  \n",
      "Loss: 3.480 — Avg: 3.497: 100%|██████████| 5000/5000 [03:17<00:00, 25.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m                        \n",
      "Loss: 3.480 — Avg: 3.497: 100%|██████████| 5000/5000 [03:17<00:00, 25.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========                                                                   \n",
      "Loss: 3.480 — Avg: 3.497: 100%|██████████| 5000/5000 [03:17<00:00, 25.26it/s]\n",
      ",                                                                            \n",
      "Give me bads.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Both you see the wastard.\n",
      "\n",
      "BRUTUS:\n",
      "Sof thou not been,\n",
      "Wheret though not.\n",
      "\n",
      "BRIAN:\n",
      "Well, peace\n",
      "Loss: 3.480 — Avg: 3.497: 100%|██████████| 5000/5000 [03:17<00:00, 25.26it/s]\n",
      "==========                                                                   \n",
      "Loss: 3.480 — Avg: 3.497: 100%|██████████| 5000/5000 [03:17<00:00, 25.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.480 — Avg: 3.497: 100%|██████████| 5000/5000 [03:17<00:00, 25.26it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "ai.train(data, batch_size=8, num_steps=5000, generate_every=5000, save_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mROMEO\u001b[0m:\n",
      "That I hearing instruptance to foul bird,\n",
      "And, and so much as your countrymberry\n",
      "Rting wint to the streams\n",
      "But in the care with his best.\n",
      "\n",
      "POMPEY\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "How now, I have told me but I do affoided.\n",
      "\n",
      "\n",
      "SEXETER:\n",
      "We traitor, I have.\n",
      "\n",
      "ISABELLA:\n",
      "It is.\n",
      "\n",
      "AUFID:\n",
      "I cannot be, sir.\n",
      "\n",
      "S\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "Mark, you, my lord, or hagillop,\n",
      "And strangerousin.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "FromISTell:\n",
      "See, mistress, good Comino!\n",
      "\n",
      "\n",
      "ESSICINIUS:\n",
      "I am\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "No, I have you were nothing.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "What, my lord?\n",
      "\n",
      "GLOUCESTER:\n",
      "Advant, sir, good for I was too?\n",
      "\n",
      "First Servingman:\n",
      "I have be my hand a pity,\n",
      "And you not,\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "I know not, not yet,\n",
      "Sirtu' the heavens, as I peace,\n",
      "For what 'twas London, finds and not\n",
      "Or empted corse,\n",
      "In such ancient sins:\n",
      "Well\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "My lord, I'll play,\n",
      "Will not to see the crowning streign ste.\n",
      "\n",
      "Third Citizen:\n",
      "Why, if I came to fealse; and I see\n",
      "Either,\n",
      "Prongs, if it be done\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "I pray you a fight; I am a victent;\n",
      "We'll kind, not soldierly, and, and I have crain'd\n",
      "To make them a pardon to be grant;\n",
      "And I am traise.\n",
      "\n",
      "\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "The hours groan to make fell.\n",
      "\n",
      "MISTALERS:\n",
      "But, I say, I were between my name,\n",
      "At accession.\n",
      "\n",
      "DORD:\n",
      "All:\n",
      "My lord, if it doth\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "Ay, this is a sight.\n",
      "\n",
      "CLAUDIO:\n",
      "Frift, Tranio!\n",
      "How can you all the stons?\n",
      "\n",
      "First Server:\n",
      "Pound! the state.\n",
      "\n",
      "BRUTUS:\n",
      "P\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "See I have been a balt to bedge.\n",
      "\n",
      "POMPEY:\n",
      "Marry, they are well;\n",
      "No, this is cousin,\n",
      "I do so hither than to know that viding\n",
      "And not the cause of the\n"
     ]
    }
   ],
   "source": [
    "ai.generate(10, prompt=\"ROMEO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
